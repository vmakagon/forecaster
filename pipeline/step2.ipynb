{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makag\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning:\n",
      "\n",
      "The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, \\\n",
    "            GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score, classification_report, \\\n",
    "            accuracy_score, precision_score, recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import optuna  # for optuna need sklearn ver less then '0.22.1' (for example 0.22)\n",
    "from optuna.samplers import RandomSampler\n",
    "from optuna.integration import OptunaSearchCV\n",
    "from optuna.distributions import CategoricalDistribution, IntUniformDistribution, \\\n",
    "    UniformDistribution, LogUniformDistribution\n",
    "from optuna.visualization import plot_slice, plot_contour, plot_optimization_history, \\\n",
    "    plot_intermediate_values, plot_parallel_coordinate\n",
    "\n",
    "SEED = 24\n",
    "N_JOBS = -1\n",
    "cv_number = 3\n",
    "cv = StratifiedKFold(n_splits=cv_number, shuffle=True, random_state=SEED)\n",
    "\n",
    "metric = 'roc_auc'\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'logreg': LogisticRegression(max_iter=100, random_state=SEED, n_jobs=N_JOBS),\n",
    "    'knn': KNeighborsClassifier(n_jobs=N_JOBS),\n",
    "    'dt': DecisionTreeClassifier(random_state=SEED),\n",
    "    'rf': RandomForestClassifier(random_state=SEED, n_jobs=N_JOBS),\n",
    "    'ext': ExtraTreesClassifier(random_state=SEED, n_jobs=N_JOBS),\n",
    "#     'bag': BaggingClassifier() # Dublicate RandomForest\n",
    "    'adb': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), random_state=SEED),\n",
    "    'gb': GradientBoostingClassifier(random_state=SEED),\n",
    "    'xgb': XGBClassifier(random_state=SEED, n_jobs=N_JOBS),\n",
    "#     'svc': SVC(random_state=SEED),\n",
    "    'cat': CatBoostClassifier(random_state=SEED, verbose=False),\n",
    "    'lgb': LGBMClassifier(random_state=SEED, n_jobs=N_JOBS),\n",
    "    'mlp': MLPClassifier(hidden_layer_sizes=(128, 32, 2), random_state=SEED),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dirs for saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check this path \"models/base\", now it is exist\n",
      "Check this path \"models/tunning\", now it is exist\n"
     ]
    }
   ],
   "source": [
    "path_for_base_model = 'models/base'\n",
    "path_for_base_model1 = 'models/base1'\n",
    "path_for_tunning_model = 'models/tunning'\n",
    "file_name_log = 'models/df_log.pkl'\n",
    "def create_dir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        if os.path.isdir(path):\n",
    "            print(f'Create dir \"{path}\"')\n",
    "    else:\n",
    "        print(f'Check this path \"{path}\", now it is exist')\n",
    "\n",
    "create_dir(path_for_base_model)\n",
    "create_dir(path_for_tunning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I see two branches for next Imputation\n",
    "* fill miising values for some Locations with ML algoritm\n",
    "* del all Locations with a lot missing values\n",
    "\n",
    "\n",
    "then compare\n",
    "\n",
    "\n",
    "* ### param for tunning each model (to choose gridCV and randomizedCV, scopeCV ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipeLine Preprocessing data after imputation\n",
    "* ### check some ways for preprocessing (Location and windDir as Categorical (one_hot) or as Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col_for_scaling = ['Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'MinTemp', \\\n",
    "                       'MaxTemp', 'Temp9am', 'Temp3pm', 'Rainfall', 'WindGustSpeed',  'WindSpeed9am',\\\n",
    "                       'WindSpeed3pm',]\n",
    "num_col_raw = ['year', 'month', 'season']\n",
    "num_question = ['Cloud9am', 'Cloud3pm', 'Evaporation', 'Sunshine',]\n",
    "num_col_categorical = ['Location_ID']\n",
    "cat_col = ['WindGustDir', 'WindDir9am', 'WindDir3pm',] \n",
    "\n",
    "tf_only_scale = StandardScaler()\n",
    "\n",
    "tf_cat_and_num_list = [\n",
    "    ('num', StandardScaler(), num_col_for_scaling + num_col_raw + num_question),\n",
    "    ('cat', OneHotEncoder(sparse=False, dtype=np.uint8, handle_unknown='ignore'), num_col_categorical + cat_col),\n",
    "]\n",
    "tf_cat_and_num = ColumnTransformer(transformers=tf_cat_and_num_list)\n",
    "\n",
    "pipe_preprocessing = Pipeline([\n",
    "    ('tf1', tf_only_scale),\n",
    "    ('tf2', tf_cat_and_num),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data with all imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142193, 24)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/df_after_all_imputations.csv')\n",
    "df.sort_values(by=['Location_ID', 'Date'], inplace=True)\n",
    "df.reset_index(inplace=True);\n",
    "df.drop(['Unnamed: 0', 'index', 'Date'], axis=1, inplace=True)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data test for stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113754, 23) (28439, 23) (113754,) (28439,)\n"
     ]
    }
   ],
   "source": [
    "X, X_test_stacking, y, y_test_stacking = train_test_split(\n",
    "    df.drop('RainTomorrow', axis=1), \n",
    "    df['RainTomorrow'],\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    shuffle=True,\n",
    ")\n",
    "print(X.shape, X_test_stacking.shape, y.shape, y_test_stacking.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save base models with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_show = True\n",
    "print(X.shape, y.shape)\n",
    "score_dict = dict()\n",
    "for i, model in model_dict.items():\n",
    "    print(f'Start {i}...')\n",
    "    if time_show:\n",
    "        start_time = datetime.now()\n",
    "    ml_pipe = Pipeline([\n",
    "        ('pre', pipe_preprocessing),\n",
    "        ('model', model),\n",
    "    ])\n",
    "    param_gs = [{'pre__tf1': [None],},\n",
    "                {'pre__tf2': [None],},\n",
    "                {'pre__tf1': [None],'pre__tf2': [None],}]\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        estimator=ml_pipe,\n",
    "        param_grid=param_gs,\n",
    "        cv=cv,\n",
    "        scoring=metric,\n",
    "        return_train_score=False,\n",
    "        verbose=0,\n",
    "        n_jobs=N_JOBS,\n",
    "    )\n",
    "    gs.fit(X, y);\n",
    "    print(f'gs.best_params_ {gs.best_params_}')\n",
    "    print(f'gs.best_score_ {gs.best_score_}')\n",
    "#     gs.best_params_, gs.best_score_, gs.best_estimator_.score(X_train, y_train), gs.best_estimator_.score(X_test, y_test)    \n",
    "    print\n",
    "    best_model = gs.best_estimator_\n",
    "    best_model.fit(X, y)\n",
    "    y_pred = best_model.predict_proba(X)[:, 1]\n",
    "    result = {\n",
    "        'gs.best_params_': gs.best_params_,\n",
    "        'gs.best_estimator_' : gs.best_estimator_,\n",
    "        'gs.best_score_' : gs.best_score_,\n",
    "        'roc_auc_score' : roc_auc_score(y, y_pred),\n",
    "    }\n",
    "    score_dict.setdefault(i, result)\n",
    "    file_name = os.path.join(path_for_base_model, f''+ i +'-base_model-[without_drop].pkl')\n",
    "    joblib.dump(best_model, file_name)\n",
    "    if os.path.isfile(file_name):\n",
    "        print(f'Model saves as {file_name}')\n",
    "    else:\n",
    "        print(f'File doesn`t save {file_name}')\n",
    "    \n",
    "    if time_show:\n",
    "        print(datetime.now() - start_time)\n",
    "    print(f'! Finish {i}')\n",
    "    \n",
    "file_name = os.path.join(path_for_base_model, f'score_dict-base_model-[without_drop].pkl')\n",
    "joblib.dump(score_dict, file_name)\n",
    "if os.path.isfile(file_name):\n",
    "    print(f'File saves as {file_name}')\n",
    "else:\n",
    "    print(f'File doesn`t save {file_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/base1\n",
      ".ipynb_checkpoints\n",
      "adb-base_model-[without_drop].pkl\n",
      "cat-base_model-[without_drop].pkl\n",
      "del\n",
      "dt-base_model-[without_drop].pkl\n",
      "ext-base_model-[without_drop].pkl\n",
      "gb-base_model-[without_drop].pkl\n",
      "knn-base_model2-[without_drop].pkl\n",
      "lgb-base_model-[without_drop].pkl\n",
      "logreg-base_model-[without_drop].pkl\n",
      "mlp-base_model3-[without_drop].pkl\n",
      "outcome - Save base models with preprocessing.txt\n",
      "rf-base_model-[without_drop].pkl\n",
      "score_dict-base_model-[without_drop].pkl\n",
      "xgb-base_model-[without_drop].pkl\n"
     ]
    }
   ],
   "source": [
    "model_base_dict = dict()\n",
    "print(path_for_base_model1)\n",
    "for file in os.listdir(path_for_base_model1):\n",
    "    print(file)\n",
    "    name = file.split('-')[0]\n",
    "    if name in model_dict:\n",
    "        # if we found in model_dict\n",
    "        model = joblib.load(os.path.join(path_for_base_model1,file))\n",
    "        model_base_dict.setdefault(name, model)\n",
    "    elif 'score_dict' == name:\n",
    "        score_dict = joblib.load(os.path.join(path_for_base_model1,file))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113754, 23) (113754,)\n"
     ]
    }
   ],
   "source": [
    "temp = model_base_dict['logreg']\n",
    "print(X.shape, y.shape)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunning model and save, save log in pd.DF => pickle \n",
    "* ### we have best preproccessing for each model, load it, learn pipeline model and tune hyper parameters for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_file(file_name):\n",
    "    d = dict(\n",
    "        model_type=model_type,\n",
    "        model=model,\n",
    "        type_of_model='tunning', # or base\n",
    "        best_preprocessing_pipe=best_preprocessing_pipe,\n",
    "        time_save=datetime.now().strftime(\"%m-%d-%Y, %H-%M-%S\"),\n",
    "        time_search=None,\n",
    "        best_roc_auc=0.5,\n",
    "        best_hypparam=None,\n",
    "        type_of_searchers='GridSearchCV',\n",
    "        start_params_for_searchers=None,\n",
    "    )\n",
    "    df_log = pd.DataFrame(columns=d.keys())\n",
    "    joblib.dump(df_log, file_name)\n",
    "\n",
    "def write_log(data, file_name):\n",
    "    if not os.path.isfile(file_name):\n",
    "        create_log_file(file_name)\n",
    "    df_log = joblib.load(file_name)\n",
    "    df_log = pd.concat([df_log, data], ignore_index=True)\n",
    "    joblib.dump(df_log, file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "model_type = 'mlp'\n",
    "# model_dict = {\n",
    "#     'logreg': LogisticRegression(max_iter=100, random_state=SEED, n_jobs=N_JOBS),\n",
    "#     'knn': KNeighborsClassifier(n_jobs=N_JOBS),\n",
    "#     'dt': DecisionTreeClassifier(random_state=SEED),\n",
    "#     'rf': RandomForestClassifier(random_state=SEED, n_jobs=N_JOBS),\n",
    "#     'ext': ExtraTreesClassifier(random_state=SEED, n_jobs=N_JOBS),\n",
    "# #     'bag': BaggingClassifier() # Dublicate RandomForest\n",
    "#     'adb': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), random_state=SEED),\n",
    "#     'gb': GradientBoostingClassifier(random_state=SEED),\n",
    "#     'xgb': XGBClassifier(random_state=SEED, n_jobs=N_JOBS),\n",
    "# #     'svc': SVC(probability=True, random_state=SEED),\n",
    "#     'cat': CatBoostClassifier(random_state=SEED, verbose=False),\n",
    "#     'lgb': LGBMClassifier(random_state=SEED, n_jobs=N_JOBS),\n",
    "# }\n",
    "type_of_searchers='GridSearchCV'\n",
    "start_time = datetime.now()\n",
    "base_pipeline = model_base_dict[model_type]\n",
    "print(X.shape, y.shape)\n",
    "best_preprocessing_pipe = base_pipeline[0]\n",
    "model = model_dict[model_type]\n",
    "\n",
    "ml_pipe = Pipeline([\n",
    "    ('pre', best_preprocessing_pipe),\n",
    "    ('m', model),\n",
    "])\n",
    "param_gs = {\n",
    "#     'm__solver' : ['lbfgs', 'liblinear'],\n",
    "\n",
    "#     'm__penalty': ['l1', 'l2'],\n",
    "#     'm__C': [0.1, 0.47, 1, 10], # [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "}\n",
    "gs = GridSearchCV(\n",
    "    estimator=ml_pipe,\n",
    "    param_grid=param_gs,\n",
    "    cv=cv,\n",
    "    scoring=metric,\n",
    "    return_train_score=False,\n",
    "    verbose=11,\n",
    "    n_jobs=N_JOBS,\n",
    ")\n",
    "gs.fit(X, y);\n",
    "print(f'gs.best_params_ {gs.best_params_}')\n",
    "print(f'gs.best_score_ {gs.best_score_}')\n",
    "best_model = gs.best_estimator_\n",
    "best_model.fit(X, y)\n",
    "y_pred = best_model.predict_proba(X)[:, 1]\n",
    "\n",
    "cur_time = datetime.now().strftime(\"%m-%d-%Y, %H-%M-%S\")\n",
    "type_of_model = 'tunning'\n",
    "file_name = os.path.join(path_for_tunning_model, \\\n",
    "    f'{model_type}-{type_of_model}_model-[{type_of_searchers}][{cur_time}].pkl')\n",
    "joblib.dump(best_model, file_name)\n",
    "new_log = dict(\n",
    "    model_type=model_type,\n",
    "    model=file_name,\n",
    "    type_of_model=type_of_model, # or base\n",
    "    best_preprocessing_pipe=best_preprocessing_pipe,\n",
    "    time_save=cur_time,\n",
    "    time_search=datetime.now() - start_time,\n",
    "    best_roc_auc=roc_auc_score(y, y_pred),\n",
    "    best_hypparam=gs.best_params_,\n",
    "    type_of_searchers=type_of_searchers,\n",
    "    start_params_for_searchers=param_gs,\n",
    ")\n",
    "print(f'new_log[best_roc_auc] = {new_log[\"best_roc_auc\"]}')\n",
    "write_log(pd.DataFrame([new_log]), file_name_log)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
